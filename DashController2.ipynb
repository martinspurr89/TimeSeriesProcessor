{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ToOL-PRO-BES.png](Info/ToOL-PRO-BES.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.autonotebook import trange, tqdm\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pytz import timezone\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup from Info.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel(\"Info/Info.xlsx\", sheet_name=None, index_col=0)\n",
    "print(*info.keys(), sep = \" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Functions\n",
    "def set_date(date_str, old_tz, dt_format = \"%d/%m/%Y %H:%M:%S\"):\n",
    "    if date_str == 'NaT':\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        datetime_set_naive = datetime.strptime(date_str, dt_format)\n",
    "        datetime_set_old = timezone(old_tz).localize(datetime_set_naive)\n",
    "        datetime_set_utc = datetime_set_old.astimezone(timezone('UTC'))\n",
    "        return datetime_set_utc\n",
    "\n",
    "def date_parser(date_, time_, dt_format = \"%d/%m/%Y %H:%M:%S\"):\n",
    "    return set_date(date_ + \" \" + time_, 'UTC', dt_format)\n",
    "\n",
    "def date_parserYMD(date_, time_, dt_format = \"%Y-%m-%d %H:%M:%S\"):\n",
    "    return set_date(date_ + \" \" + time_, 'UTC', dt_format)\n",
    "\n",
    "\n",
    "# Experiment start/end date\n",
    "date_start = set_date(str(info['setup'].loc['date_start_utc','value']), \"UTC\", \"%Y-%m-%d %H:%M:%S\")\n",
    "if pd.isna(info['setup'].loc['date_end_utc','value']):\n",
    "    date_now = datetime.now(timezone('UTC'))\n",
    "    info['setup'].loc['date_end_utc','value'] = date_now\n",
    "else:\n",
    "    date_now = set_date(str(info['setup'].loc['date_end_utc','value']), \"UTC\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Date Functions continued\n",
    "def date_range(window=-1, start=date_start, end=date_now):\n",
    "    if pd.isna(start):\n",
    "        start = date_start\n",
    "    if pd.isna(end):\n",
    "        end = date_now\n",
    "    if window != -1:\n",
    "        if not pd.isna(window):\n",
    "            start = end - timedelta(days=window)\n",
    "    return start, end\n",
    "\n",
    "info['setup']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process Information sheets from Info.xlsx\n",
    "\n",
    "### Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['datasets']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chartxs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts = []\n",
    "for chart, row in info['charts'].iterrows():\n",
    "    chart_range = date_range(window = row['chart_range_window'], start = set_date(str(row['chart_range_start']), \"UTC\", \"%Y-%m-%d %H:%M:%S\"), end = set_date(str(row['chart_range_end']), \"UTC\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "    info['charts'].loc[chart, 'chart_range_start'] = chart_range[0]\n",
    "    info['charts'].loc[chart, 'chart_range_end'] = chart_range[1]\n",
    "    info['charts'].loc[chart, 'chart_range_window'] = chart_range[1] - chart_range[0]\n",
    "    charts.append(chart)\n",
    "\n",
    "del chart, chart_range, row\n",
    "\n",
    "info['charts']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['parameters_ave']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['plots']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Colours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['colours']\n",
    "\n",
    "info['colours']['rgb'] = list(zip((info['colours']['r']/255), (info['colours']['g']/255), (info['colours']['b']/255)))\n",
    "info['colours']['rgb_str'] = \"rgb(\" + info['colours']['r'].astype(int).astype(str) + \",\" + info['colours']['g'].astype(int).astype(str) + \",\" + info['colours']['b'].astype(int).astype(str) + \")\"\n",
    "\n",
    "colour_palettes = {}\n",
    "colour_palettes['all'] = info['colours']['rgb'].tolist()\n",
    "\n",
    "for theme in info['colours']['theme'].unique():\n",
    "    colour_palettes[theme] = info['colours'].query('theme == \"' + theme + '\"')['rgb'].tolist()\n",
    "    sns.palplot(colour_palettes[theme])\n",
    "\n",
    "del theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot([colour_palettes['mid'][i] for i in [0,3,5,6]])\n",
    "sns.palplot([colour_palettes['mid'][i] for i in [1,2,4,7]])\n",
    "sns.palplot([colour_palettes['mid'][i] for i in [8,9]])\n",
    "\n",
    "del colour_palettes"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data\n",
    "### Custom Support Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UO_support_data(dataset_f_info, dataset = \"UO\"):\n",
    "    \n",
    "    def import_UO_log_dates():\n",
    "        UO_log_dates = pd.read_csv(dataset_f_info['supp_data_filepath'][dataset], sep = \"\\t\")\n",
    "        UO_log_dates['start'] = pd.to_datetime(UO_log_dates['start'], format=\"%Y-%m-%d\")\n",
    "        UO_log_dates['end'] = pd.to_datetime(UO_log_dates['end'], format=\"%Y-%m-%d\")\n",
    "        return UO_log_dates\n",
    "    \n",
    "    UO_log_dates = import_UO_log_dates()\n",
    "    \n",
    "    #Check LogDates file\n",
    "    if max(UO_log_dates['end']) < date_now.date() - timedelta(days=1):\n",
    "        print(\"Retrieving latest Urban Observatory data...\")\n",
    "        # api-endpoint \n",
    "        URL = \"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/data/csv/?\"\n",
    "        # location given here \n",
    "        poly = '0103000000010000000600000001000020a2baf9bf58094ee3a2764b4001000020f268fbbf90c059b5b5714b4000000020ba37fbbfc4e78445c6704b4001000020b203fbbf9ce8d60e68714b40010000209a59f9bf346a0efd06714b4001000020a2baf9bf58094ee3a2764b40'\n",
    "        # defining a params dict for the parameters to be sent to the API \n",
    "        PARAMS = {'polygon_wkb': poly,\n",
    "                  'starttime': max(UO_log_dates['end']).strftime(\"%Y%m%d%H%M%S\"), #\"%Y%m%d%H%M%S\"\n",
    "                  'endtime': date_now.strftime(\"%Y%m%d%H%M%S\"), #\"%Y%m%d%H%M%S\"\n",
    "                  'data_variable': 'River Level,Temperature,Rainfall'} \n",
    "        # sending get request and saving the response as response object \n",
    "        data_content = requests.get(url = URL, params = PARAMS).content\n",
    "        UO_data = pd.read_csv(io.StringIO(data_content.decode(\"utf-8\")))\n",
    "        min_date = pd.to_datetime(min(UO_data['Timestamp'])).strftime(\"%Y-%m-%d\")\n",
    "        max_date = pd.to_datetime(max(UO_data['Timestamp'])).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        if pd.to_datetime(max_date) > max(UO_log_dates['end']):\n",
    "            UO_data.to_csv(dataset_f_info['data_folder_path'][dataset] + \"UO_data_\" + str(min_date) + \"_to_\" + str(max_date) + \".csv\",\n",
    "                           index = False)\n",
    "            UO_Files = []\n",
    "            for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])), desc = \"Open existing UO files\"):\n",
    "                if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "                    UO_Files.append(filename)\n",
    "            UO_starts = []\n",
    "            for string in UO_Files: UO_starts.append(string[8:18])\n",
    "            UO_ends = []\n",
    "            for string in UO_Files: UO_ends.append(string[22:32])\n",
    "\n",
    "\n",
    "            new_UO_file_data = {'file': UO_Files,\n",
    "                                'start': UO_starts,\n",
    "                                'end': UO_ends}\n",
    "            UO_log_dates_new = pd.DataFrame(new_UO_file_data, columns = ['file', 'start', 'end'])\n",
    "            UO_log_dates_new.to_csv(dataset_f_info['supp_data_filepath'][dataset], index = False, sep = \"\\t\")\n",
    "            print(\"Urban Observatory data updated!\")\n",
    "        else:\n",
    "            print(\"Urban Observatory data up-to-date!\")\n",
    "        \n",
    "    UO_log_dates = import_UO_log_dates\n",
    "    \n",
    "    return UO_log_dates\n",
    "\n",
    "def get_Events_support_data(dataset_f_info, dataset = \"Events\"):\n",
    "    Events_info = pd.read_csv(dataset_f_info['supp_data_filepath'][dataset], sep = \"\\t\", index_col = \"Code\")\n",
    "    return Events_info\n",
    "\n",
    "custom_preimport_functions = {'UO': get_UO_support_data,\n",
    "                              'Events': get_Events_support_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Imported Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_imported_Sensor_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset = \"Sensor\"):\n",
    "    day_df['DateTime'] = day_df['Date'] + \" \" + day_df['Time']\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format = '%d/%m/%Y %H:%M:%S')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "    \n",
    "    #Make on/off events integer binary numbers 1/0\n",
    "    #on_events = [\"^ON$\", \"^RUNNING$\", \"^CW$\", \"^AUTO$\"]\n",
    "    #off_events = [\"^OFF$\", \"^STOP$\", \"^CCW$\", \"^MANUAL$\"]\n",
    "\n",
    "    #all_rgx = re.compile(\"|\".join(on_events + off_events))\n",
    "    #on_rgx = re.compile(\"|\".join(on_events))\n",
    "    #off_rgx = re.compile(\"|\".join(off_events))\n",
    "    \n",
    "    #Sensor_events = {'ON': 1,\n",
    "    #                 'RUNNING': 1,\n",
    "    #                 'CW': 1,\n",
    "    #                 'AUTO': 1,\n",
    "    #                 'OFF': 0,\n",
    "    #                 'STOP': 0,\n",
    "    #                 'CCW': 0,\n",
    "    #                 'MANUAL': 0}\n",
    "\n",
    "    #string_cols = (day_df.applymap(type) == str).all(0).values\n",
    "    #for col in day_df.iloc[:, string_cols]:\n",
    "    #    if day_df[col].str.contains(all_rgx).any():\n",
    "    #        day_df[col] = day_df[col].replace(on_rgx, '1', regex=True)\n",
    "    #        day_df[col] = day_df[col].replace(off_rgx, '0', regex=True)\n",
    "    #        day_df[col] = day_df[col].astype(int)\n",
    "    #for col in day_df.iloc[:, string_cols]:\n",
    "    #if day_df.iloc[:, string_cols].str.contains(all_rgx).any():\n",
    "    #day_df[[i for i in list(day_df.columns) if i not in ['DateTime', 'Date', 'Time']]] = \\\n",
    "    #day_df[[i for i in list(day_df.columns) if i not in ['DateTime', 'Date', 'Time']]].replace(Sensor_events).apply(pd.to_numeric)\n",
    "    #day_df.iloc[:, string_cols] = day_df.astype(int)\n",
    "    \n",
    "    #Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"__C\")]/(math.pi*(0.6)**2*10)\n",
    "    #Sum Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")]/4\n",
    "    \n",
    "    #Sensible tilt (up = positive)\n",
    "    day_df['SENSOR_TILT'] = -day_df['SENSOR_TILT']\n",
    "\n",
    "    return(day_df)\n",
    "\n",
    "def mod_imported_Skid_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset = \"Skid\"):\n",
    "    day_df = day_df.rename(columns={\"TIME\": \"DateTime\"})\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format = '%Y/%m/%d %H:%M:%S.%f')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "    selected_pars = list(info['parameters'][selected].query('dataset == \"' + dataset + '\"')['parameter'].values)\n",
    "    day_df = day_df[day_df[selected_pars].sum(axis = 1, skipna = True) != 0]\n",
    "    return(day_df)\n",
    "\n",
    "def mod_imported_SampLog_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset = \"SampLog\"):\n",
    "    day_df = day_df.rename(columns={\"Date\": \"DateTime\"})\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format = '%d/%m/%Y %H:%M')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('Europe/London')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_convert('UTC')\n",
    "\n",
    "    selected_read_cols = ['R1', 'R2', 'R3']\n",
    "    day_df[selected_read_cols] = day_df[selected_read_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    #Average readings\n",
    "    day_df['Read_ave'] = day_df[selected_read_cols].mean(axis = 1, skipna = True)\n",
    "    \n",
    "    #Widen DF\n",
    "    day_df_wide = pd.pivot_table(day_df,values='Read_ave',index=['DateTime', 'Location'],columns=['Type','Vial'])\n",
    "    #Fix col header and names\n",
    "    day_df_wide.columns = map(''.join, (str(v) for v in day_df_wide.columns))\n",
    "    day_df_wide.columns = [re.sub(r'\\W', '', i) for i in day_df_wide.columns]\n",
    "    day_df_wide.columns = [s[:len(s)-1] + \"_\" + s[len(s)-1:] for s in day_df_wide.columns]\n",
    "    day_df_wide = day_df_wide.reset_index()\n",
    "    \n",
    "    return(day_df_wide)\n",
    "\n",
    "def mod_imported_NWL_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset = \"NWL\"):\n",
    "    day_df['DateTime'] = day_df['DATE'].astype(str) + \" \" + day_df['TIME']\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format = '%Y-%m-%d %H:%M')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('Europe/London')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_convert('UTC')\n",
    "    \n",
    "    day_df = day_df[day_df['SITE'].str.contains('SETTLED')]\n",
    "    \n",
    "    return(day_df)\n",
    "\n",
    "def mod_imported_UO_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset = \"UO\"):\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['Timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "  \n",
    "    # Create Type column\n",
    "    day_df['Type'] = day_df['Sensor Name'].astype(str) + \"_\" + day_df['Variable']\n",
    "  \n",
    "    #Widen DF\n",
    "    day_df_wide = pd.pivot_table(day_df,values='Value',index=['DateTime'],columns=['Type'])\n",
    "    #Fix col header and names\n",
    "    day_df_wide = day_df_wide.reset_index()\n",
    "    day_df_wide = day_df_wide.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'], \n",
    "                                                info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'])))\n",
    "    return(day_df_wide)\n",
    "\n",
    "custom_import_functions = {'Sensor': mod_imported_Sensor_data,\n",
    "                           'Skid': mod_imported_Skid_data,\n",
    "                           'SampLog': mod_imported_SampLog_data,\n",
    "                           'NWL': mod_imported_NWL_data,\n",
    "                           'UO': mod_imported_UO_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data import\n",
    "def import_data(dataset):\n",
    "    dataset_all_days = []\n",
    "    for folder in range(1,len(info['datasets'].query('dataset == \"' + dataset + '\"'))+1):\n",
    "        print(dataset + \": Folder \" + str(folder))\n",
    "        \n",
    "        #Shorthand info:\n",
    "        dataset_f_info = info['datasets'].query('dataset == \"' + dataset + '\" & folder == \"' + str(folder) + '\"')\n",
    "        #File format to import\n",
    "        f_file_import = select_file_importer(dataset, dataset_f_info)\n",
    "        \n",
    "        #Custom pre-import functions\n",
    "        if dataset in custom_preimport_functions:\n",
    "            dataset_supp_data = custom_preimport_functions[dataset](dataset_f_info)\n",
    "        else:\n",
    "            dataset_supp_data = \"\"\n",
    "        #Import files\n",
    "        for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])), desc = \"Open files to import\"):\n",
    "            if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "                day_df = file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import)\n",
    "                # Keep if longer than 0 lines\n",
    "                if len(day_df) > 0:\n",
    "                    #Keep if within DateTime range in Info file\n",
    "                    if max(day_df['DateTime']) >= min(info['charts']['chart_range_start']) and \\\n",
    "                       min(day_df['DateTime']) <= max(info['charts']['chart_range_end']):\n",
    "                        dataset_all_days.append(day_df)\n",
    "    \n",
    "    #Create one dataframe from all days data\n",
    "    dataset_all_data = pd.concat(dataset_all_days, axis=0, ignore_index=True)\n",
    "    dataset_all_data.sort_values(by=['DateTime'], inplace=True)\n",
    "    dataset_all_data = dataset_all_data.reset_index(drop=True)\n",
    "    \n",
    "    dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] >= min(info['charts']['chart_range_start'])]\n",
    "    dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] <= max(info['charts']['chart_range_end'])]\n",
    "    \n",
    "    return(dataset_all_data)\n",
    "\n",
    "#File Import functions\n",
    "def select_file_importer(dataset, dataset_f_info):\n",
    "    if re.search(\"xls\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"xls\"]\n",
    "    elif re.search(\"csv\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"csv\"]\n",
    "    elif re.search(\"txt\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"txt\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown file pattern!\")\n",
    "\n",
    "def file_import_xls(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_excel(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]), \n",
    "                       skiprows = dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "def file_import_csv(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_csv(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]), \n",
    "                     skiprows = dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "def file_import_txt(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_csv(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]), \n",
    "                     sep=\"\\t\", skiprows = dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "file_import_functions = {'xls': file_import_xls,\n",
    "                         'csv': file_import_csv,\n",
    "                         'txt': file_import_txt}\n",
    "\n",
    "def file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import):\n",
    "    day_df = f_file_import(dataset, dataset_f_info, filename)\n",
    "    if len(day_df) > 0:\n",
    "        # Name parameter columns\n",
    "        day_df = day_df.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'], \n",
    "                                                info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'])))\n",
    "        # Delete unit rows\n",
    "        if not pd.isna(dataset_f_info['Del_unit_rows'][dataset]):\n",
    "            day_df = day_df.drop(0).reset_index()\n",
    "        ##Load custom data mod function?\n",
    "        if dataset in custom_import_functions:\n",
    "            day_df = custom_import_functions[dataset](day_df, filename, dataset_supp_data, dataset_f_info, dataset)\n",
    "        \n",
    "        #Choose selected parameters\n",
    "        selected_pars = list(info['parameters'][selected].query('dataset == \"' + dataset + '\"')['parameter'].values)\n",
    "        day_df = day_df[['DateTime'] + selected_pars]\n",
    "        #Add blank row between files - to be implemented\n",
    "        #if len(day_df) > 0:\n",
    "        #    if not pd.isna(dataset_f_info['Add_blank_rows'][dataset]):\n",
    "        #        day_df = day_df.append(pd.Series(), ignore_index=True)\n",
    "                \n",
    "\n",
    "    return(day_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [s for s in info['parameters'].columns.to_list() if \"selected\" in s]\n",
    "selected = info['parameters'][selected_cols].isin([1]).any(axis=1)\n",
    "selected_datasets = list(selected[selected == True].index.unique())\n",
    "selected_datasets\n",
    "\n",
    "del selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data = []\n",
    "for dataset in tqdm(selected_datasets, desc = \"Import data from each dataset\"):\n",
    "    dataset_data.append(import_data(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(dataset_data, axis=0, ignore_index=True, sort=False)\n",
    "all_data.sort_values(by=['DateTime'], inplace=True)\n",
    "cols = ['DateTime']  + [col for col in all_data if col != 'DateTime']\n",
    "all_data = all_data[cols]\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "del selected_datasets, selected, cols, dataset_data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    warnings.filterwarnings('ignore', r'Degrees of freedom <= 0 for slice.')\n",
    "    ave_cols = []\n",
    "    for col in all_data.columns[1:].to_list():\n",
    "        ave_col = info['parameters'].query('parameter == \"' + col + '\"')['parameter_ave'][0]\n",
    "        if ave_col != col:\n",
    "            if ave_col not in ave_cols:\n",
    "                cols = info['parameters'].query('parameter_ave == \"' + ave_col + '\"')['parameter'].to_list()\n",
    "                all_data[ave_col] = all_data[cols].mean(axis=1)\n",
    "                if len(cols) > 2:\n",
    "                    all_data[ave_col + \"_err\"] = np.nanstd(all_data[cols], axis=1)\n",
    "                elif len(cols) == 2:\n",
    "                    all_data[ave_col + \"_err\"] = np.abs((all_data[cols[0]] - all_data[cols[1]])/2)\n",
    "                else:\n",
    "                    all_data[ave_col + \"_err\"] = 0\n",
    "                ave_cols.append(info['parameters'].query('parameter == \"' + col + '\"')['parameter_ave'][0])\n",
    "\n",
    "del ave_col, col, ave_cols, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_dfs = []\n",
    "for chart in tqdm(charts, desc = \"Assign data to chart DFs\"):\n",
    "    if info['charts'].loc[chart, 'chart_status'] == 'ON':\n",
    "        mask = (all_data['DateTime'] >= info['charts'].loc[chart, 'chart_range_start']) & (all_data['DateTime'] <= info['charts'].loc[chart, 'chart_range_end'])\n",
    "        df = all_data.loc[mask]\n",
    "        if info['charts'].loc[chart, 'chart_res'] != 0:\n",
    "            df = df.resample(\"\".join([str(info['charts'].loc[chart, 'chart_res']), 'T']), on='DateTime').mean()\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "        chart_dfs.append(df)\n",
    "    else:\n",
    "        chart_dfs.append(\"\")\n",
    "\n",
    "del df, chart, mask, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_dfs_mlt = []\n",
    "for chart in charts:\n",
    "    if info['charts'].loc[chart, 'chart_status'] == 'ON':\n",
    "        data_cols = []\n",
    "        err_cols = [\"DateTime\"]\n",
    "        for col in chart_dfs[chart].columns:\n",
    "            if not \"_err\" in col:\n",
    "                data_cols.append(col)\n",
    "            else:\n",
    "                err_cols.append(col)\n",
    "\n",
    "        df = chart_dfs[chart][data_cols].melt(id_vars=['DateTime'], var_name='Parameter', value_name='Value')\n",
    "        df_err = chart_dfs[chart][err_cols].melt(id_vars=['DateTime'], var_name='Parameter', value_name='Error')\n",
    "        \n",
    "        df = df.set_index(['DateTime', 'Parameter', df.groupby(['DateTime', 'Parameter']).cumcount()])\n",
    "        df_err['Parameter'] = df_err['Parameter'].str.replace(r'_err', '')\n",
    "        df_err = df_err.set_index(['DateTime', 'Parameter', df_err.groupby(['DateTime', 'Parameter']).cumcount()])\n",
    "        \n",
    "        df3 = (pd.concat([df, df_err],axis=1)\n",
    "        .sort_index(level=2)\n",
    "        .reset_index(level=2, drop=True)\n",
    "        .reset_index())\n",
    "        df3.sort_values(by=['DateTime', 'Parameter'], inplace=True)\n",
    "\n",
    "        chart_dfs_mlt.append(df3)\n",
    "    else:\n",
    "        chart_dfs_mlt.append(\"\")\n",
    "\n",
    "del chart, data_cols, err_cols, col, df, df_err, df3, chart_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\n",
    "df['D'] = df['U'].map(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_dfs_mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pars = []\n",
    "for chart in charts:\n",
    "    if info['charts'].loc[chart, 'chart_status'] == 'ON':\n",
    "        par_info1 = info['parameters'][info['parameters']['parameter'].isin(chart_dfs_mlt[chart].Parameter.unique())].drop(columns=[\"code\", \"parameter_ave\"])\n",
    "        par_info2 = info['parameters_ave'][info['parameters_ave']['parameter_ave'].isin(chart_dfs_mlt[chart].Parameter.unique())].rename(columns={\"parameter_ave\": \"parameter\"})\n",
    "        par_info_all = (par_info1.append(par_info2)).query('selected_chart_' + str(chart) + ' == 1')\n",
    "\n",
    "        #Convert colour id to rgb string\n",
    "        par_info_all['colour'].replace(info['colours']['rgb_str'].to_dict(), inplace=True)\n",
    "        par_info_all['fill'].replace(info['colours']['rgb_str'].to_dict(), inplace=True)\n",
    "\n",
    "        #Set defaults for NAs\n",
    "        par_info_all['colour'].fillna(info['colours'].query(\"theme == 'dark'\")['rgb_str'].to_list()[0], inplace=True)\n",
    "        par_info_all['fill'].fillna(info['colours'].query(\"theme == 'dark'\")['rgb_str'].to_list()[0], inplace=True)\n",
    "        par_info_all['shape'].fillna(1, inplace=True)\n",
    "        par_info_all['line_type'].fillna(\"solid\", inplace=True)\n",
    "        par_info_all['show_in_legend'].fillna(True, inplace=True)\n",
    "        \n",
    "        plot_pars.append(par_info_all)\n",
    "    else:\n",
    "        plot_pars.append(\"\")\n",
    "plot_pars\n",
    "\n",
    "del chart, par_info1, par_info2, par_info_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_figs = []\n",
    "for chart in charts:\n",
    "    if info['charts'].loc[chart, 'chart_status'] == 'ON':\n",
    "        chart_figs.append([])\n",
    "        fig = go.Figure()\n",
    "        for i in range(0, len(chart_dfs_mlt[chart].Parameter.unique())):\n",
    "            par = chart_dfs_mlt[chart].Parameter.unique()[i]\n",
    "            plot_par = plot_pars[chart].query('parameter == \"' + par + '\"')\n",
    "            x_data = chart_dfs_mlt[chart][chart_dfs_mlt[chart].Parameter == par].DateTime\n",
    "            y_data = chart_dfs_mlt[chart][chart_dfs_mlt[chart].Parameter == par].Value\n",
    "            y_error = chart_dfs_mlt[chart][chart_dfs_mlt[chart].Parameter == par].Error\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines',\n",
    "                                    name=plot_par['parameter_lab'][0], \n",
    "                                    line=dict(color=plot_par['colour'][0], width=1),\n",
    "                                    connectgaps=True, legendgroup=plot_par['parameter_lab'][0], showlegend=bool(plot_par['show_in_legend'][0])\n",
    "                                    ))\n",
    "            chart_figs[chart].append(fig)\n",
    "    else:\n",
    "        chart_figs.append(\"\")\n",
    "\n",
    "del chart, fig, i, par, plot_par, x_data, y_data, y_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_figs[0][0].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "250px",
    "left": "1330px",
    "right": "20px",
    "top": "109px",
    "width": "575px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}