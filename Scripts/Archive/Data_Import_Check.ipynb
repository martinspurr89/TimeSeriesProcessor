{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdashenvcondaafc3fd1ed224455ca3aee86bd709bff6",
   "display_name": "Python 3.7.6 64-bit ('DashEnv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import packages\n",
    "import datetime\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pytz import timezone\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Setup from Info.xlsx\n",
    "info = pd.read_excel(\"Info/Info.xlsx\", sheet_name=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Functions\n",
    "# Date Functions\n",
    "def set_date(date_str, old_tz, dt_format=\"%d/%m/%Y %H:%M:%S\"):\n",
    "    if date_str == 'NaT':\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        datetime_set_naive = datetime.strptime(date_str, dt_format)\n",
    "        datetime_set_old = timezone(old_tz).localize(datetime_set_naive)\n",
    "        datetime_set_utc = datetime_set_old.astimezone(timezone('UTC'))\n",
    "        return datetime_set_utc\n",
    "\n",
    "def date_parser(date_, time_, dt_format=\"%d/%m/%Y %H:%M:%S\"):\n",
    "    return set_date(date_ + \" \" + time_, 'UTC', dt_format)\n",
    "\n",
    "\n",
    "def date_parserYMD(date_, time_, dt_format=\"%Y-%m-%d %H:%M:%S\"):\n",
    "    return set_date(date_ + \" \" + time_, 'UTC', dt_format)\n",
    "\n",
    "\n",
    "# Experiment start/end date\n",
    "date_start = set_date(str(info['setup'].loc['date_start_utc', 'value']), \"UTC\", \"%Y-%m-%d %H:%M:%S\")\n",
    "if pd.isna(info['setup'].loc['date_end_utc', 'value']):\n",
    "    date_now = datetime.now(timezone('UTC'))\n",
    "    info['setup'].loc['date_end_utc', 'value'] = date_now\n",
    "else:\n",
    "    date_now = set_date(str(info['setup'].loc['date_end_utc', 'value']), \"UTC\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# Date Functions continued\n",
    "def date_range(window=-1, start=date_start, end=date_now):\n",
    "    if pd.isna(start):\n",
    "        start = date_start\n",
    "    if pd.isna(end):\n",
    "        end = date_now\n",
    "    if window != -1:\n",
    "        if not pd.isna(window):\n",
    "            start = end - timedelta(days=window)\n",
    "    return start, end\n",
    "\n",
    "\n",
    "# ## Process Information sheets from Info.xlsx\n",
    "# ### Chartxs:\n",
    "charts = []\n",
    "for chart, row in info['charts'].iterrows():\n",
    "    chart_range = date_range(window=row['chart_range_window'],\n",
    "                             start=set_date(str(row['chart_range_start']), \"UTC\", \"%Y-%m-%d %H:%M:%S\"),\n",
    "                             end=set_date(str(row['chart_range_end']), \"UTC\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "    info['charts'].loc[chart, 'chart_range_start'] = chart_range[0]\n",
    "    info['charts'].loc[chart, 'chart_range_end'] = chart_range[1]\n",
    "    info['charts'].loc[chart, 'chart_range_window'] = chart_range[1] - chart_range[0]\n",
    "    charts.append(chart)\n",
    "\n",
    "del chart, chart_range, row\n",
    "\n",
    "# ### Colours:\n",
    "\n",
    "info['colours']['rgb'] = list(\n",
    "    zip((info['colours']['r'] / 255), (info['colours']['g'] / 255), (info['colours']['b'] / 255)))\n",
    "info['colours']['rgba_str'] = \"rgba(\" + info['colours']['r'].astype(int).astype(str) + \",\" + info['colours']['g'].astype(\n",
    "    int).astype(str) + \",\" + info['colours']['b'].astype(int).astype(str) + \",1)\"\n",
    "\n",
    "\n",
    "# ## Import Data\n",
    "# ### Custom Support Data Functions\n",
    "\n",
    "def get_UO_support_data(dataset_f_info, dataset=\"UO\"):\n",
    "    def import_UO_log_dates():\n",
    "        UO_log_dates = pd.read_csv(dataset_f_info['supp_data_filepath'][dataset], sep=\"\\t\")\n",
    "        UO_log_dates['start'] = pd.to_datetime(UO_log_dates['start'], format=\"%Y-%m-%d\")\n",
    "        UO_log_dates['end'] = pd.to_datetime(UO_log_dates['end'], format=\"%Y-%m-%d\")\n",
    "        return UO_log_dates\n",
    "\n",
    "    UO_log_dates = import_UO_log_dates()\n",
    "\n",
    "    # Check LogDates file\n",
    "    if max(UO_log_dates['end']) < date_now.date() - timedelta(days=1):\n",
    "        # print(\"Retrieving latest Urban Observatory data...\")\n",
    "        # api-endpoint\n",
    "        URL = \"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/data/csv/?\"\n",
    "        # location given here\n",
    "        poly = '0103000000010000000600000001000020a2baf9bf58094ee3a2764b4001000020f268fbbf90c059b5b5714b4000000020ba37fbbfc4e78445c6704b4001000020b203fbbf9ce8d60e68714b40010000209a59f9bf346a0efd06714b4001000020a2baf9bf58094ee3a2764b40'\n",
    "        # defining a params dict for the parameters to be sent to the API\n",
    "        PARAMS = {'polygon_wkb': poly,\n",
    "                  'starttime': max(UO_log_dates['end']).strftime(\"%Y%m%d%H%M%S\"),  # \"%Y%m%d%H%M%S\"\n",
    "                  'endtime': date_now.strftime(\"%Y%m%d%H%M%S\"),  # \"%Y%m%d%H%M%S\"\n",
    "                  'data_variable': 'River Level,Temperature,Rainfall'}\n",
    "        # sending get request and saving the response as response object\n",
    "        data_content = requests.get(url=URL, params=PARAMS).content\n",
    "        UO_data = pd.read_csv(io.StringIO(data_content.decode(\"utf-8\")))\n",
    "        min_date = pd.to_datetime(min(UO_data['Timestamp'])).strftime(\"%Y-%m-%d\")\n",
    "        max_date = pd.to_datetime(max(UO_data['Timestamp'])).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if pd.to_datetime(max_date) > max(UO_log_dates['end']):\n",
    "            UO_data.to_csv(dataset_f_info['data_folder_path'][dataset] + \"UO_data_\" + str(min_date) + \"_to_\" + str(\n",
    "                max_date) + \".csv\",\n",
    "                           index=False)\n",
    "            UO_Files = []\n",
    "            for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])),\n",
    "                                 desc=\"Open existing UO files\"):\n",
    "                if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "                    UO_Files.append(filename)\n",
    "            UO_starts = []\n",
    "            for string in UO_Files: UO_starts.append(string[8:18])\n",
    "            UO_ends = []\n",
    "            for string in UO_Files: UO_ends.append(string[22:32])\n",
    "\n",
    "            new_UO_file_data = {'file': UO_Files,\n",
    "                                'start': UO_starts,\n",
    "                                'end': UO_ends}\n",
    "            UO_log_dates_new = pd.DataFrame(new_UO_file_data, columns=['file', 'start', 'end'])\n",
    "            UO_log_dates_new.to_csv(dataset_f_info['supp_data_filepath'][dataset], index=False, sep=\"\\t\")\n",
    "            print(\"Urban Observatory data updated!\")\n",
    "        else:\n",
    "            print(\"Urban Observatory data up-to-date!\")\n",
    "\n",
    "    UO_log_dates = import_UO_log_dates\n",
    "\n",
    "    return UO_log_dates\n",
    "\n",
    "\n",
    "def get_Events_support_data(dataset_f_info, dataset=\"Events\"):\n",
    "    Events_info = pd.read_csv(dataset_f_info['supp_data_filepath'][dataset], sep=\"\\t\", index_col=\"Code\")\n",
    "    return Events_info\n",
    "\n",
    "\n",
    "custom_preimport_functions = {'UO': get_UO_support_data,\n",
    "                              'Events': get_Events_support_data}\n",
    "\n",
    "\n",
    "# ### Custom Imported Data Functions\n",
    "def mod_imported_Sensor_py_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"Sensor\"):\n",
    "    day_df['DateTime'] = day_df['Date'] + \" \" + day_df['Time']\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%d/%m/%y %H:%M:%S')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"__C\")] / (\n",
    "            math.pi * 0.6 ** 2 * 10)\n",
    "    # Sum Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] / 4\n",
    "\n",
    "    return day_df\n",
    "\n",
    "\n",
    "def mod_imported_Skid_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"Skid\"):\n",
    "    day_df = day_df.rename(columns={\"TIME\": \"DateTime\"})\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%Y/%m/%d %H:%M:%S.%f')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "    selected_pars = list(info['parameters'][selected].query('dataset == \"' + dataset + '\"')['parameter'].values)\n",
    "    day_df = day_df[day_df[selected_pars].sum(axis=1, skipna=True) != 0]\n",
    "    return day_df\n",
    "\n",
    "\n",
    "def mod_imported_SampLog_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"SampLog\"):\n",
    "    day_df = day_df.rename(columns={\"Date\": \"DateTime\"})\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%d/%m/%Y %H:%M')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('Europe/London')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_convert('UTC')\n",
    "\n",
    "    selected_read_cols = ['R1', 'R2', 'R3']\n",
    "    day_df[selected_read_cols] = day_df[selected_read_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Average readings\n",
    "    day_df['Read_ave'] = day_df[selected_read_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "    # Widen DF\n",
    "    day_df_wide = pd.pivot_table(day_df, values='Read_ave', index=['DateTime', 'Location'], columns=['Type', 'Vial'])\n",
    "    # Fix col header and names\n",
    "    day_df_wide.columns = map(''.join, (str(v) for v in day_df_wide.columns))\n",
    "    day_df_wide.columns = [re.sub(r'\\W', '', i) for i in day_df_wide.columns]\n",
    "    day_df_wide.columns = [s[:len(s) - 1] + \"_\" + s[len(s) - 1:] for s in day_df_wide.columns]\n",
    "    day_df_wide = day_df_wide.reset_index()\n",
    "\n",
    "    return day_df_wide\n",
    "\n",
    "\n",
    "def mod_imported_NWL_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"NWL\"):\n",
    "    day_df['DateTime'] = day_df['DATE'].astype(str) + \" \" + day_df['TIME']\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%Y-%m-%d %H:%M')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('Europe/London')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_convert('UTC')\n",
    "\n",
    "    day_df = day_df[day_df['SITE'].str.contains('SETTLED')]\n",
    "\n",
    "    return day_df\n",
    "\n",
    "\n",
    "def mod_imported_UO_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"UO\"):\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['Timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Create Type column\n",
    "    day_df['Type'] = day_df['Sensor Name'].astype(str) + \"_\" + day_df['Variable']\n",
    "\n",
    "    # Widen DF\n",
    "    day_df_wide = pd.pivot_table(day_df, values='Value', index=['DateTime'], columns=['Type'])\n",
    "    # Fix col header and names\n",
    "    day_df_wide = day_df_wide.reset_index()\n",
    "    day_df_wide = day_df_wide.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'],\n",
    "                                                      info['parameters'].query('dataset == \"' + dataset + '\"')[\n",
    "                                                          'parameter'])))\n",
    "    return day_df_wide\n",
    "\n",
    "\n",
    "custom_import_functions = {'Sensor_py': mod_imported_Sensor_py_data,\n",
    "                           'Skid': mod_imported_Skid_data,\n",
    "                           'SampLog': mod_imported_SampLog_data,\n",
    "                           'NWL': mod_imported_NWL_data,\n",
    "                           'UO': mod_imported_UO_data}\n",
    "\n",
    "\n",
    "# ### Data Import Functions\n",
    "# Data import\n",
    "def import_data(dataset):\n",
    "    dataset_all_days = []\n",
    "    for folder in range(1, len(info['datasets'].query('dataset == \"' + dataset + '\"')) + 1):\n",
    "        # print(dataset + \": Folder \" + str(folder))\n",
    "\n",
    "        # Shorthand info:\n",
    "        dataset_f_info = info['datasets'].query('dataset == \"' + dataset + '\" & folder == \"' + str(folder) + '\"')\n",
    "        # File format to import\n",
    "        f_file_import = select_file_importer(dataset, dataset_f_info)\n",
    "\n",
    "        # Custom pre-import functions\n",
    "        if dataset in custom_preimport_functions:\n",
    "            dataset_supp_data = custom_preimport_functions[dataset](dataset_f_info)\n",
    "        else:\n",
    "            dataset_supp_data = \"\"\n",
    "        # Import files\n",
    "        for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])), desc=\"Open files to import\"):\n",
    "            if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "                day_df = file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import)\n",
    "                # Keep if longer than 0 lines\n",
    "                if len(day_df) > 0:\n",
    "                    # Keep if within DateTime range in Info file\n",
    "                    if max(day_df['DateTime']) >= min(info['charts']['chart_range_start']) and min(\n",
    "                            day_df['DateTime']) <= max(info['charts']['chart_range_end']):\n",
    "                        dataset_all_days.append(day_df)\n",
    "\n",
    "    # Create one dataframe from all days data\n",
    "    dataset_all_data = pd.concat(dataset_all_days, axis=0, ignore_index=True)\n",
    "    dataset_all_data.sort_values(by=['DateTime'], inplace=True)\n",
    "    dataset_all_data = dataset_all_data.reset_index(drop=True)\n",
    "\n",
    "    dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] >= min(info['charts']['chart_range_start'])]\n",
    "    dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] <= max(info['charts']['chart_range_end'])]\n",
    "\n",
    "    return dataset_all_data\n",
    "\n",
    "\n",
    "# File Import functions\n",
    "def select_file_importer(dataset, dataset_f_info):\n",
    "    if re.search(\"xls\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"xls\"]\n",
    "    elif re.search(\"csv\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"csv\"]\n",
    "    elif re.search(\"txt\", dataset_f_info['file_pat'][dataset], re.IGNORECASE):\n",
    "        return file_import_functions[\"txt\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown file pattern!\")\n",
    "\n",
    "\n",
    "def file_import_xls(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_excel(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]),\n",
    "                       skiprows=dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "\n",
    "def file_import_csv(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_csv(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]),\n",
    "                     skiprows=dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "\n",
    "def file_import_txt(dataset, dataset_f_info, filename):\n",
    "    df = pd.read_csv(\"\".join([str(dataset_f_info['data_folder_path'][dataset]), filename]),\n",
    "                     sep=\"\\t\", skiprows=dataset_f_info['skiprows'][dataset])\n",
    "    return df\n",
    "\n",
    "\n",
    "file_import_functions = {'xls': file_import_xls,\n",
    "                         'csv': file_import_csv,\n",
    "                         'txt': file_import_txt}\n",
    "\n",
    "\n",
    "def file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import):\n",
    "    day_df = f_file_import(dataset, dataset_f_info, filename)\n",
    "    if len(day_df) > 0:\n",
    "        # Name parameter columns\n",
    "        day_df = day_df.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'],\n",
    "                                                info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'])))\n",
    "        # Delete unit rows\n",
    "        if not pd.isna(dataset_f_info['Del_unit_rows'][dataset]):\n",
    "            day_df = day_df.drop(0).reset_index()\n",
    "        ##Load custom data mod function?\n",
    "        if dataset in custom_import_functions:\n",
    "            day_df = custom_import_functions[dataset](day_df, filename, dataset_supp_data, dataset_f_info, dataset)\n",
    "\n",
    "        #Choose parameters included in parameters sheet\n",
    "        selected_pars = list(info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'].values)\n",
    "        day_df = day_df[['DateTime'] + selected_pars].copy() #Copy prevents hidden chain indexing later\n",
    "\n",
    "        #Make floats\n",
    "        for col in selected_pars:\n",
    "            try:\n",
    "                day_df.loc[:,col] = day_df.loc[:,col].astype(float)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add blank row between files - to be implemented\n",
    "        # if len(day_df) > 0:\n",
    "        #    if not pd.isna(dataset_f_info['Add_blank_rows'][dataset]):\n",
    "        #        day_df = day_df.append(pd.Series(), ignore_index=True)\n",
    "\n",
    "    return day_df\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Import data from each dataset:   0%|          | 0/5 [00:00<?, ?it/s]\nOpen files to import: 100%|██████████| 2/2 [00:00<00:00, 34.08it/s]\n\nOpen files to import:   0%|          | 0/371 [00:00<?, ?it/s]\u001b[A\nOpen files to import:   1%|          | 3/371 [00:00<00:14, 25.73it/s]\u001b[A\nOpen files to import:   2%|▏         | 7/371 [00:00<00:14, 25.32it/s]\u001b[A\nOpen files to import:   3%|▎         | 11/371 [00:00<00:12, 28.14it/s]\u001b[A\nOpen files to import:   4%|▍         | 15/371 [00:00<00:12, 29.29it/s]\u001b[A\nOpen files to import:   5%|▌         | 19/371 [00:00<00:11, 29.62it/s]\u001b[A\nOpen files to import:   6%|▌         | 23/371 [00:00<00:11, 31.32it/s]\u001b[A\nOpen files to import:   7%|▋         | 26/371 [00:00<00:11, 30.01it/s]\u001b[A\nOpen files to import:   8%|▊         | 30/371 [00:00<00:11, 30.82it/s]\u001b[A\nOpen files to import:   9%|▉         | 33/371 [00:01<00:11, 30.37it/s]\u001b[A\nOpen files to import:  10%|▉         | 36/371 [00:01<00:11, 29.42it/s]\u001b[A\nOpen files to import:  11%|█         | 40/371 [00:01<00:11, 29.82it/s]\u001b[A\nOpen files to import:  12%|█▏        | 43/371 [00:01<00:11, 29.11it/s]\u001b[A\nOpen files to import:  12%|█▏        | 46/371 [00:01<00:11, 29.05it/s]\u001b[A\nOpen files to import:  13%|█▎        | 49/371 [00:01<00:11, 28.97it/s]\u001b[A\nOpen files to import:  14%|█▍        | 52/371 [00:01<00:11, 28.41it/s]\u001b[A\nOpen files to import:  15%|█▍        | 55/371 [00:01<00:11, 28.69it/s]\u001b[A\nOpen files to import:  16%|█▌        | 58/371 [00:01<00:11, 26.59it/s]\u001b[A\nOpen files to import:  16%|█▋        | 61/371 [00:02<00:11, 25.89it/s]\u001b[A\nOpen files to import:  17%|█▋        | 64/371 [00:02<00:11, 26.10it/s]\u001b[A\nOpen files to import:  18%|█▊        | 68/371 [00:02<00:11, 27.16it/s]\u001b[A\nOpen files to import:  19%|█▉        | 71/371 [00:02<00:11, 26.47it/s]\u001b[A\nOpen files to import:  20%|█▉        | 74/371 [00:02<00:10, 27.20it/s]\u001b[A\nOpen files to import:  21%|██        | 78/371 [00:02<00:10, 28.40it/s]\u001b[A\nOpen files to import:  22%|██▏       | 82/371 [00:02<00:10, 28.85it/s]\u001b[A\nOpen files to import:  23%|██▎       | 85/371 [00:02<00:10, 26.98it/s]\u001b[A\nOpen files to import:  24%|██▎       | 88/371 [00:03<00:10, 27.37it/s]\u001b[A\nOpen files to import:  25%|██▍       | 91/371 [00:03<00:10, 26.37it/s]\u001b[A\nOpen files to import:  25%|██▌       | 94/371 [00:03<00:10, 27.17it/s]\u001b[A\nOpen files to import:  26%|██▋       | 98/371 [00:03<00:09, 28.97it/s]\u001b[A\nOpen files to import:  28%|██▊       | 103/371 [00:03<00:08, 33.15it/s]\u001b[A\nOpen files to import:  29%|██▉       | 108/371 [00:03<00:07, 34.85it/s]\u001b[A\nOpen files to import:  30%|███       | 112/371 [00:03<00:07, 34.41it/s]\u001b[A\nOpen files to import:  31%|███▏      | 116/371 [00:03<00:07, 34.05it/s]\u001b[A\nOpen files to import:  32%|███▏      | 120/371 [00:03<00:07, 34.40it/s]\u001b[A\nOpen files to import:  33%|███▎      | 124/371 [00:04<00:07, 33.89it/s]\u001b[A\nOpen files to import:  35%|███▍      | 128/371 [00:04<00:07, 33.63it/s]\u001b[A\nOpen files to import:  36%|███▌      | 132/371 [00:04<00:07, 32.84it/s]\u001b[A\nOpen files to import:  37%|███▋      | 136/371 [00:04<00:07, 33.27it/s]\u001b[A\nOpen files to import:  38%|███▊      | 140/371 [00:04<00:07, 32.92it/s]\u001b[A\nOpen files to import:  39%|███▉      | 144/371 [00:04<00:07, 32.15it/s]\u001b[A\nOpen files to import:  40%|████      | 149/371 [00:04<00:06, 35.02it/s]\u001b[A\nOpen files to import:  42%|████▏     | 154/371 [00:04<00:05, 37.47it/s]\u001b[A\nOpen files to import:  43%|████▎     | 158/371 [00:05<00:05, 35.84it/s]\u001b[A\nOpen files to import:  44%|████▎     | 162/371 [00:05<00:06, 34.59it/s]\u001b[A\nOpen files to import:  45%|████▍     | 166/371 [00:05<00:06, 32.94it/s]\u001b[A\nOpen files to import:  46%|████▌     | 170/371 [00:05<00:06, 33.12it/s]\u001b[A\nOpen files to import:  47%|████▋     | 174/371 [00:05<00:05, 33.31it/s]\u001b[A\nOpen files to import:  48%|████▊     | 178/371 [00:05<00:05, 34.44it/s]\u001b[A\nOpen files to import:  49%|████▉     | 182/371 [00:05<00:05, 33.84it/s]\u001b[A\nOpen files to import:  50%|█████     | 186/371 [00:05<00:05, 33.65it/s]\u001b[A\nOpen files to import:  51%|█████     | 190/371 [00:06<00:05, 33.49it/s]\u001b[A\nOpen files to import:  52%|█████▏    | 194/371 [00:06<00:05, 33.67it/s]\u001b[A\nOpen files to import:  53%|█████▎    | 198/371 [00:06<00:05, 33.61it/s]\u001b[A\nOpen files to import:  54%|█████▍    | 202/371 [00:06<00:05, 33.46it/s]\u001b[A\nOpen files to import:  56%|█████▌    | 206/371 [00:06<00:04, 33.41it/s]\u001b[A\nOpen files to import:  57%|█████▋    | 210/371 [00:06<00:04, 33.45it/s]\u001b[A\nOpen files to import:  58%|█████▊    | 214/371 [00:06<00:04, 32.36it/s]\u001b[A\nOpen files to import:  59%|█████▉    | 218/371 [00:06<00:04, 32.99it/s]\u001b[A\nOpen files to import:  60%|█████▉    | 222/371 [00:07<00:04, 32.91it/s]\u001b[A\nOpen files to import:  61%|██████    | 226/371 [00:07<00:04, 32.31it/s]\u001b[A\nOpen files to import:  62%|██████▏   | 230/371 [00:07<00:04, 32.37it/s]\u001b[A\nOpen files to import:  63%|██████▎   | 234/371 [00:07<00:04, 32.48it/s]\u001b[A\nOpen files to import:  64%|██████▍   | 238/371 [00:07<00:04, 32.32it/s]\u001b[A\nOpen files to import:  65%|██████▌   | 242/371 [00:07<00:03, 32.33it/s]\u001b[A\nOpen files to import:  66%|██████▋   | 246/371 [00:07<00:03, 32.16it/s]\u001b[A\nOpen files to import:  67%|██████▋   | 250/371 [00:07<00:03, 32.48it/s]\u001b[A\nOpen files to import:  68%|██████▊   | 254/371 [00:08<00:03, 30.66it/s]\u001b[A\nOpen files to import:  70%|██████▉   | 258/371 [00:08<00:03, 30.63it/s]\u001b[A\nOpen files to import:  71%|███████   | 262/371 [00:08<00:03, 31.18it/s]\u001b[A\nOpen files to import:  72%|███████▏  | 266/371 [00:08<00:03, 31.19it/s]\u001b[A\nOpen files to import:  73%|███████▎  | 270/371 [00:08<00:03, 31.34it/s]\u001b[A\nOpen files to import:  74%|███████▍  | 274/371 [00:08<00:03, 31.73it/s]\u001b[A\nOpen files to import:  75%|███████▍  | 278/371 [00:08<00:02, 31.54it/s]\u001b[A\nOpen files to import:  76%|███████▌  | 282/371 [00:08<00:02, 31.60it/s]\u001b[A\nOpen files to import:  77%|███████▋  | 286/371 [00:09<00:02, 31.33it/s]\u001b[A\nOpen files to import:  78%|███████▊  | 290/371 [00:09<00:02, 32.32it/s]\u001b[A\nOpen files to import:  79%|███████▉  | 294/371 [00:09<00:03, 24.96it/s]\u001b[A\nOpen files to import:  80%|████████  | 297/371 [00:09<00:02, 26.16it/s]\u001b[A\nOpen files to import:  81%|████████  | 300/371 [00:09<00:02, 27.05it/s]\u001b[A\nOpen files to import:  82%|████████▏ | 303/371 [00:09<00:02, 27.48it/s]\u001b[A\nOpen files to import:  82%|████████▏ | 306/371 [00:09<00:02, 24.42it/s]\u001b[A\nOpen files to import:  83%|████████▎ | 309/371 [00:09<00:02, 25.20it/s]\u001b[A\nOpen files to import:  84%|████████▍ | 312/371 [00:10<00:02, 25.43it/s]\u001b[A\nOpen files to import:  85%|████████▍ | 315/371 [00:10<00:02, 25.39it/s]\u001b[A\nOpen files to import:  86%|████████▌ | 318/371 [00:10<00:02, 25.72it/s]\u001b[A\nOpen files to import:  87%|████████▋ | 321/371 [00:10<00:01, 26.43it/s]\u001b[A\nOpen files to import:  87%|████████▋ | 324/371 [00:10<00:01, 25.53it/s]\u001b[A\nOpen files to import:  88%|████████▊ | 327/371 [00:10<00:01, 26.46it/s]\u001b[A\nOpen files to import:  89%|████████▉ | 331/371 [00:10<00:01, 27.82it/s]\u001b[A\nOpen files to import:  90%|█████████ | 334/371 [00:10<00:01, 25.51it/s]\u001b[A\nOpen files to import:  91%|█████████ | 337/371 [00:11<00:01, 26.18it/s]\u001b[A\nOpen files to import:  92%|█████████▏| 340/371 [00:11<00:01, 26.52it/s]\u001b[A\nOpen files to import:  92%|█████████▏| 343/371 [00:11<00:01, 26.95it/s]\u001b[A\nOpen files to import:  93%|█████████▎| 346/371 [00:11<00:00, 27.15it/s]\u001b[A\nOpen files to import:  94%|█████████▍| 349/371 [00:11<00:00, 24.68it/s]\u001b[A\nOpen files to import:  95%|█████████▍| 352/371 [00:11<00:00, 24.86it/s]\u001b[A\nOpen files to import:  96%|█████████▌| 355/371 [00:11<00:00, 25.76it/s]\u001b[A\nOpen files to import:  96%|█████████▋| 358/371 [00:11<00:00, 23.81it/s]\u001b[A\nOpen files to import:  97%|█████████▋| 361/371 [00:12<00:00, 24.10it/s]\u001b[A\nOpen files to import:  98%|█████████▊| 364/371 [00:12<00:00, 25.06it/s]\u001b[A\nOpen files to import: 100%|██████████| 371/371 [00:12<00:00, 30.19it/s]\nImport data from each dataset:  40%|████      | 2/5 [00:12<00:18,  6.20s/it]\nOpen files to import:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\nOpen files to import:  12%|█▎        | 1/8 [00:01<00:12,  1.73s/it]\u001b[A\nOpen files to import:  50%|█████     | 4/8 [00:01<00:04,  1.23s/it]\u001b[A\nOpen files to import: 100%|██████████| 8/8 [00:02<00:00,  3.94it/s]\nImport data from each dataset:  60%|██████    | 3/5 [00:14<00:09,  4.95s/it]\nOpen files to import:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[AUrban Observatory data up-to-date!\n\nOpen files to import: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\nImport data from each dataset:  80%|████████  | 4/5 [00:24<00:06,  6.41s/it]\nOpen files to import: 100%|██████████| 4/4 [00:00<00:00, 92.54it/s]\nImport data from each dataset: 100%|██████████| 5/5 [00:24<00:00,  4.86s/it]\n"
    }
   ],
   "source": [
    "# ### Data Import\n",
    "selected_cols = [s for s in info['parameters'].columns.to_list() if \"selected\" in s]\n",
    "selected = info['parameters'][selected_cols].isin([1]).any(axis=1)\n",
    "\n",
    "selected_cols_ave = [s for s in info['parameters_ave'].columns.to_list() if \"selected\" in s]\n",
    "selected_ave = info['parameters_ave'][selected_cols_ave].isin([1]).any(axis=1)\n",
    "\n",
    "selected_datasets = list(selected[selected].index.unique().values) + list(selected_ave[selected_ave].index.unique().values)\n",
    "selected_datasets = [x for i, x in enumerate(selected_datasets) if i == selected_datasets.index(x)]\n",
    "\n",
    "del selected_cols, selected_cols_ave\n",
    "\n",
    "dataset_data = []\n",
    "for dataset in tqdm(selected_datasets, desc = \"Import data from each dataset\"):\n",
    "    dataset_data.append(import_data(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Open files to import: 100%|██████████| 2/2 [00:00<00:00, 78.95it/s]\n"
    }
   ],
   "source": [
    "dataset = 'Sensor_py'\n",
    "# Data import\n",
    "dataset_all_days = []\n",
    "folder = 1\n",
    "# Shorthand info:\n",
    "dataset_f_info = info['datasets'].query('dataset == \"' + dataset + '\" & folder == \"' + str(folder) + '\"')\n",
    "# File format to import\n",
    "f_file_import = select_file_importer(dataset, dataset_f_info)\n",
    "\n",
    "# Custom pre-import functions\n",
    "if dataset in custom_preimport_functions:\n",
    "    dataset_supp_data = custom_preimport_functions[dataset](dataset_f_info)\n",
    "else:\n",
    "    dataset_supp_data = \"\"\n",
    "for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])), desc=\"Open files to import\"):\n",
    "    if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "        day_df = f_file_import(dataset, dataset_f_info, filename)\n",
    "        if len(day_df) > 0:\n",
    "            # Name parameter columns\n",
    "            day_df = day_df.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'],\n",
    "                                                    info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'])))\n",
    "            # Delete unit rows\n",
    "            if not pd.isna(dataset_f_info['Del_unit_rows'][dataset]):\n",
    "                day_df = day_df.drop(0).reset_index()\n",
    "            ##Load custom data mod function?\n",
    "            day_df['DateTime'] = day_df['Date'] + \" \" + day_df['Time']\n",
    "            day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%d/%m/%y %H:%M:%S')\n",
    "            day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "\n",
    "            # Current density\n",
    "            day_df.iloc[:, day_df.columns.str.contains(\"__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"__C\")] / (\n",
    "                    math.pi * 0.6 ** 2 * 10)\n",
    "            # Sum Current density\n",
    "            day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] / 4\n",
    "\n",
    "            # Sensible tilt (up = positive)\n",
    "            day_df['SENSOR_TILT'] = -day_df['SENSOR_TILT']\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         Date      Time  SENSOR_pH    TEMP_OS  SENSOR_CONDUCTIVITY  \\\n0    09/04/20  10:21:14   7.143036  13.424844           970.489502   \n1    09/04/20  10:22:14   7.143019  13.435362           961.170898   \n2    09/04/20  10:23:16   7.143019  13.395898           975.869568   \n3    09/04/20  10:24:16   7.142791  13.465779           985.639954   \n4    09/04/20  10:25:18   7.144160  13.395038           976.029480   \n..        ...       ...        ...        ...                  ...   \n102  09/04/20  12:26:20   7.139498  13.803379           966.054871   \n103  09/04/20  12:27:22   7.138814  13.796021           961.389343   \n104  09/04/20  12:28:22   7.139042  13.783340           975.292786   \n105  09/04/20  12:29:24   7.138814  13.776778           981.307739   \n106  09/04/20  12:30:26   7.138359  13.790141           970.504028   \n\n      BES_A1__V  BES_A2__V   BES_A3__V  BES_A4__V  TEMP_HOSE_A  ...  \\\n0    123.686670  64.672873  111.365108  78.939945    36.880495  ...   \n1    126.280683  66.942635  115.580379  80.236951    42.833708  ...   \n2    126.929186  66.942635  116.228882  81.533958    47.794718  ...   \n3    126.929186  66.942635  115.256128  81.858210    47.794718  ...   \n4    126.280683  66.618383  115.256128  80.885455    48.786920  ...   \n..          ...        ...         ...        ...          ...  ...   \n102  135.035477  67.266886  123.686670  82.506713    52.755728  ...   \n103  135.683981  67.591138  124.659425  82.506713    49.779122  ...   \n104  135.359729  67.266886  124.335173  81.858210    56.724537  ...   \n105  135.035477  66.942635  123.686670  82.506713    46.802516  ...   \n106  134.711226  66.942635  123.362418  82.182461    54.740133  ...   \n\n     BES_C3__C  BES_C4__C  BES_A_SUM__V  BES_B_SUM__V  BES_C_SUM__V  \\\n0     3.827273   3.583088    378.664596    475.940086    379.637351   \n1     3.999638   3.683635    389.040649    489.882906    393.580171   \n2     3.999638   3.697999    391.634662    496.367939    394.552926   \n3     3.942183   3.669271    390.986158    495.070933    390.986158   \n4     3.927820   3.654907    389.040649    493.773926    390.661907   \n..         ...        ...           ...           ...           ...   \n102   4.373097   4.344370    408.495747    520.038309    445.460433   \n103   4.416189   4.401825    410.441256    522.632322    449.675704   \n104   4.387461   4.387461    408.819998    522.308070    447.730194   \n105   4.373097   4.387461    408.171495    520.038309    446.433187   \n106   4.344370   4.373097    407.198740    521.011064    446.757439   \n\n     BES_A_SUM__C  BES_B_SUM__C  BES_C_SUM__C  REFRESH_WAIT  \\\n0        4.193550      5.270835      4.204323           250   \n1        4.308460      5.425245      4.358733           190   \n2        4.337188      5.497064      4.369506           128   \n3        4.330006      5.482701      4.330006            68   \n4        4.308460      5.468337      4.326415           296   \n..            ...           ...           ...           ...   \n102      4.523917      5.759204      4.933285            40   \n103      4.545463      5.787931      4.979968           278   \n104      4.527508      5.784340      4.958422           218   \n105      4.520326      5.759204      4.944058           156   \n106      4.509553      5.769976      4.947649            94   \n\n                     DateTime  \n0   2020-04-09 10:21:14+00:00  \n1   2020-04-09 10:22:14+00:00  \n2   2020-04-09 10:23:16+00:00  \n3   2020-04-09 10:24:16+00:00  \n4   2020-04-09 10:25:18+00:00  \n..                        ...  \n102 2020-04-09 12:26:20+00:00  \n103 2020-04-09 12:27:22+00:00  \n104 2020-04-09 12:28:22+00:00  \n105 2020-04-09 12:29:24+00:00  \n106 2020-04-09 12:30:26+00:00  \n\n[107 rows x 66 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>SENSOR_pH</th>\n      <th>TEMP_OS</th>\n      <th>SENSOR_CONDUCTIVITY</th>\n      <th>BES_A1__V</th>\n      <th>BES_A2__V</th>\n      <th>BES_A3__V</th>\n      <th>BES_A4__V</th>\n      <th>TEMP_HOSE_A</th>\n      <th>...</th>\n      <th>BES_C3__C</th>\n      <th>BES_C4__C</th>\n      <th>BES_A_SUM__V</th>\n      <th>BES_B_SUM__V</th>\n      <th>BES_C_SUM__V</th>\n      <th>BES_A_SUM__C</th>\n      <th>BES_B_SUM__C</th>\n      <th>BES_C_SUM__C</th>\n      <th>REFRESH_WAIT</th>\n      <th>DateTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>09/04/20</td>\n      <td>10:21:14</td>\n      <td>7.143036</td>\n      <td>13.424844</td>\n      <td>970.489502</td>\n      <td>123.686670</td>\n      <td>64.672873</td>\n      <td>111.365108</td>\n      <td>78.939945</td>\n      <td>36.880495</td>\n      <td>...</td>\n      <td>3.827273</td>\n      <td>3.583088</td>\n      <td>378.664596</td>\n      <td>475.940086</td>\n      <td>379.637351</td>\n      <td>4.193550</td>\n      <td>5.270835</td>\n      <td>4.204323</td>\n      <td>250</td>\n      <td>2020-04-09 10:21:14+00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09/04/20</td>\n      <td>10:22:14</td>\n      <td>7.143019</td>\n      <td>13.435362</td>\n      <td>961.170898</td>\n      <td>126.280683</td>\n      <td>66.942635</td>\n      <td>115.580379</td>\n      <td>80.236951</td>\n      <td>42.833708</td>\n      <td>...</td>\n      <td>3.999638</td>\n      <td>3.683635</td>\n      <td>389.040649</td>\n      <td>489.882906</td>\n      <td>393.580171</td>\n      <td>4.308460</td>\n      <td>5.425245</td>\n      <td>4.358733</td>\n      <td>190</td>\n      <td>2020-04-09 10:22:14+00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>09/04/20</td>\n      <td>10:23:16</td>\n      <td>7.143019</td>\n      <td>13.395898</td>\n      <td>975.869568</td>\n      <td>126.929186</td>\n      <td>66.942635</td>\n      <td>116.228882</td>\n      <td>81.533958</td>\n      <td>47.794718</td>\n      <td>...</td>\n      <td>3.999638</td>\n      <td>3.697999</td>\n      <td>391.634662</td>\n      <td>496.367939</td>\n      <td>394.552926</td>\n      <td>4.337188</td>\n      <td>5.497064</td>\n      <td>4.369506</td>\n      <td>128</td>\n      <td>2020-04-09 10:23:16+00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>09/04/20</td>\n      <td>10:24:16</td>\n      <td>7.142791</td>\n      <td>13.465779</td>\n      <td>985.639954</td>\n      <td>126.929186</td>\n      <td>66.942635</td>\n      <td>115.256128</td>\n      <td>81.858210</td>\n      <td>47.794718</td>\n      <td>...</td>\n      <td>3.942183</td>\n      <td>3.669271</td>\n      <td>390.986158</td>\n      <td>495.070933</td>\n      <td>390.986158</td>\n      <td>4.330006</td>\n      <td>5.482701</td>\n      <td>4.330006</td>\n      <td>68</td>\n      <td>2020-04-09 10:24:16+00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>09/04/20</td>\n      <td>10:25:18</td>\n      <td>7.144160</td>\n      <td>13.395038</td>\n      <td>976.029480</td>\n      <td>126.280683</td>\n      <td>66.618383</td>\n      <td>115.256128</td>\n      <td>80.885455</td>\n      <td>48.786920</td>\n      <td>...</td>\n      <td>3.927820</td>\n      <td>3.654907</td>\n      <td>389.040649</td>\n      <td>493.773926</td>\n      <td>390.661907</td>\n      <td>4.308460</td>\n      <td>5.468337</td>\n      <td>4.326415</td>\n      <td>296</td>\n      <td>2020-04-09 10:25:18+00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>09/04/20</td>\n      <td>12:26:20</td>\n      <td>7.139498</td>\n      <td>13.803379</td>\n      <td>966.054871</td>\n      <td>135.035477</td>\n      <td>67.266886</td>\n      <td>123.686670</td>\n      <td>82.506713</td>\n      <td>52.755728</td>\n      <td>...</td>\n      <td>4.373097</td>\n      <td>4.344370</td>\n      <td>408.495747</td>\n      <td>520.038309</td>\n      <td>445.460433</td>\n      <td>4.523917</td>\n      <td>5.759204</td>\n      <td>4.933285</td>\n      <td>40</td>\n      <td>2020-04-09 12:26:20+00:00</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>09/04/20</td>\n      <td>12:27:22</td>\n      <td>7.138814</td>\n      <td>13.796021</td>\n      <td>961.389343</td>\n      <td>135.683981</td>\n      <td>67.591138</td>\n      <td>124.659425</td>\n      <td>82.506713</td>\n      <td>49.779122</td>\n      <td>...</td>\n      <td>4.416189</td>\n      <td>4.401825</td>\n      <td>410.441256</td>\n      <td>522.632322</td>\n      <td>449.675704</td>\n      <td>4.545463</td>\n      <td>5.787931</td>\n      <td>4.979968</td>\n      <td>278</td>\n      <td>2020-04-09 12:27:22+00:00</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>09/04/20</td>\n      <td>12:28:22</td>\n      <td>7.139042</td>\n      <td>13.783340</td>\n      <td>975.292786</td>\n      <td>135.359729</td>\n      <td>67.266886</td>\n      <td>124.335173</td>\n      <td>81.858210</td>\n      <td>56.724537</td>\n      <td>...</td>\n      <td>4.387461</td>\n      <td>4.387461</td>\n      <td>408.819998</td>\n      <td>522.308070</td>\n      <td>447.730194</td>\n      <td>4.527508</td>\n      <td>5.784340</td>\n      <td>4.958422</td>\n      <td>218</td>\n      <td>2020-04-09 12:28:22+00:00</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>09/04/20</td>\n      <td>12:29:24</td>\n      <td>7.138814</td>\n      <td>13.776778</td>\n      <td>981.307739</td>\n      <td>135.035477</td>\n      <td>66.942635</td>\n      <td>123.686670</td>\n      <td>82.506713</td>\n      <td>46.802516</td>\n      <td>...</td>\n      <td>4.373097</td>\n      <td>4.387461</td>\n      <td>408.171495</td>\n      <td>520.038309</td>\n      <td>446.433187</td>\n      <td>4.520326</td>\n      <td>5.759204</td>\n      <td>4.944058</td>\n      <td>156</td>\n      <td>2020-04-09 12:29:24+00:00</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>09/04/20</td>\n      <td>12:30:26</td>\n      <td>7.138359</td>\n      <td>13.790141</td>\n      <td>970.504028</td>\n      <td>134.711226</td>\n      <td>66.942635</td>\n      <td>123.362418</td>\n      <td>82.182461</td>\n      <td>54.740133</td>\n      <td>...</td>\n      <td>4.344370</td>\n      <td>4.373097</td>\n      <td>407.198740</td>\n      <td>521.011064</td>\n      <td>446.757439</td>\n      <td>4.509553</td>\n      <td>5.769976</td>\n      <td>4.947649</td>\n      <td>94</td>\n      <td>2020-04-09 12:30:26+00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>107 rows × 66 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "filename = os.listdir(str(dataset_f_info['data_folder_path'][dataset]))[0]\n",
    "print(re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Open files to import: 100%|██████████| 2/2 [00:00<00:00, 45.31it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-71-7fcdcd8e87f3>, line 35)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-7fcdcd8e87f3>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    return dataset_all_data\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Data import\n",
    "dataset_all_days = []\n",
    "for folder in range(1, len(info['datasets'].query('dataset == \"' + dataset + '\"')) + 1):\n",
    "    # print(dataset + \": Folder \" + str(folder))\n",
    "\n",
    "    # Shorthand info:\n",
    "    dataset_f_info = info['datasets'].query('dataset == \"' + dataset + '\" & folder == \"' + str(folder) + '\"')\n",
    "    # File format to import\n",
    "    f_file_import = select_file_importer(dataset, dataset_f_info)\n",
    "\n",
    "    # Custom pre-import functions\n",
    "    if dataset in custom_preimport_functions:\n",
    "        dataset_supp_data = custom_preimport_functions[dataset](dataset_f_info)\n",
    "    else:\n",
    "        dataset_supp_data = \"\"\n",
    "    # Import files\n",
    "    for filename in tqdm(os.listdir(str(dataset_f_info['data_folder_path'][dataset])), desc=\"Open files to import\"):\n",
    "        if re.search(dataset_f_info['file_pat'][dataset], filename) and not filename.startswith('.'):\n",
    "            day_df = file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import)\n",
    "            # Keep if longer than 0 lines\n",
    "            if len(day_df) > 0:\n",
    "                # Keep if within DateTime range in Info file\n",
    "                if max(day_df['DateTime']) >= min(info['charts']['chart_range_start']) and min(\n",
    "                        day_df['DateTime']) <= max(info['charts']['chart_range_end']):\n",
    "                    dataset_all_days.append(day_df)\n",
    "\n",
    "# Create one dataframe from all days data\n",
    "dataset_all_data = pd.concat(dataset_all_days, axis=0, ignore_index=True)\n",
    "dataset_all_data.sort_values(by=['DateTime'], inplace=True)\n",
    "dataset_all_data = dataset_all_data.reset_index(drop=True)\n",
    "\n",
    "dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] >= min(info['charts']['chart_range_start'])]\n",
    "dataset_all_data = dataset_all_data[dataset_all_data['DateTime'] <= max(info['charts']['chart_range_end'])]\n",
    "\n",
    "return dataset_all_data\n",
    "\n",
    "def file_import_handler(dataset, dataset_f_info, filename, dataset_supp_data, f_file_import):\n",
    "    day_df = f_file_import(dataset, dataset_f_info, filename)\n",
    "    if len(day_df) > 0:\n",
    "        # Name parameter columns\n",
    "        day_df = day_df.rename(columns=dict(zip(info['parameters'].query('dataset == \"' + dataset + '\"')['code'],\n",
    "                                                info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'])))\n",
    "        # Delete unit rows\n",
    "        if not pd.isna(dataset_f_info['Del_unit_rows'][dataset]):\n",
    "            day_df = day_df.drop(0).reset_index()\n",
    "        ##Load custom data mod function?\n",
    "        if dataset in custom_import_functions:\n",
    "            day_df = custom_import_functions[dataset](day_df, filename, dataset_supp_data, dataset_f_info, dataset)\n",
    "\n",
    "        #Choose parameters included in parameters sheet\n",
    "        selected_pars = list(info['parameters'].query('dataset == \"' + dataset + '\"')['parameter'].values)\n",
    "        day_df = day_df[['DateTime'] + selected_pars].copy() #Copy prevents hidden chain indexing later\n",
    "\n",
    "        #Make floats\n",
    "        for col in selected_pars:\n",
    "            try:\n",
    "                day_df.loc[:,col] = day_df.loc[:,col].astype(float)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add blank row between files - to be implemented\n",
    "        # if len(day_df) > 0:\n",
    "        #    if not pd.isna(dataset_f_info['Add_blank_rows'][dataset]):\n",
    "        #        day_df = day_df.append(pd.Series(), ignore_index=True)\n",
    "\n",
    "    return day_df\n",
    "\n",
    "\n",
    "\n",
    "def mod_imported_Sensor_data(day_df, filename, dataset_supp_data, dataset_f_info, dataset=\"Sensor\"):\n",
    "    day_df['DateTime'] = day_df['Date'] + \" \" + day_df['Time']\n",
    "    day_df['DateTime'] = pd.to_datetime(day_df['DateTime'], format='%d/%m/%Y %H:%M:%S')\n",
    "    day_df['DateTime'] = day_df['DateTime'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Make on/off events integer binary numbers 1/0\n",
    "    # on_events = [\"^ON$\", \"^RUNNING$\", \"^CW$\", \"^AUTO$\"]\n",
    "    # off_events = [\"^OFF$\", \"^STOP$\", \"^CCW$\", \"^MANUAL$\"]\n",
    "\n",
    "    # all_rgx = re.compile(\"|\".join(on_events + off_events))\n",
    "    # on_rgx = re.compile(\"|\".join(on_events))\n",
    "    # off_rgx = re.compile(\"|\".join(off_events))\n",
    "\n",
    "    # Sensor_events = {'ON': 1,\n",
    "    #                 'RUNNING': 1,\n",
    "    #                 'CW': 1,\n",
    "    #                 'AUTO': 1,\n",
    "    #                 'OFF': 0,\n",
    "    #                 'STOP': 0,\n",
    "    #                 'CCW': 0,\n",
    "    #                 'MANUAL': 0}\n",
    "\n",
    "    # string_cols = (day_df.applymap(type) == str).all(0).values\n",
    "    # for col in day_df.iloc[:, string_cols]:\n",
    "    #    if day_df[col].str.contains(all_rgx).any():\n",
    "    #        day_df[col] = day_df[col].replace(on_rgx, '1', regex=True)\n",
    "    #        day_df[col] = day_df[col].replace(off_rgx, '0', regex=True)\n",
    "    #        day_df[col] = day_df[col].astype(int)\n",
    "    # for col in day_df.iloc[:, string_cols]:\n",
    "    # if day_df.iloc[:, string_cols].str.contains(all_rgx).any():\n",
    "    # day_df[[i for i in list(day_df.columns) if i not in ['DateTime', 'Date', 'Time']]] = \\\n",
    "    # day_df[[i for i in list(day_df.columns) if i not in ['DateTime', 'Date', 'Time']]].replace(Sensor_events).apply(pd.to_numeric)\n",
    "    # day_df.iloc[:, string_cols] = day_df.astype(int)\n",
    "\n",
    "    # Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"__C\")] / (\n",
    "            math.pi * 0.6 ** 2 * 10)\n",
    "    # Sum Current density\n",
    "    day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] = day_df.iloc[:, day_df.columns.str.contains(\"SUM__C\")] / 4\n",
    "\n",
    "    # Sensible tilt (up = positive)\n",
    "    day_df['SENSOR_TILT'] = -day_df['SENSOR_TILT']\n",
    "\n",
    "    return day_df"
   ]
  }
 ]
}